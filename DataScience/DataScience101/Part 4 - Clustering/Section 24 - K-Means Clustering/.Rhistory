ungroup()
mediana<-median(df$V1)
cond<-dfvar[dfvar$VAR>3]
df<-pres[pres$WORK_ID==id,]
dfvar <- df %>%
arrange(DATE_RIFE) %>%
mutate(VAR = round(rollapply(V1, width = 3, FUN = sd, align = "center", fill = NA, by = 1),5),
MEAN = round(rollapply(V1, width = 3, FUN = mean, align = "center", fill = NA, by = 1),5)) %>%
ungroup()
mediana<-median(df$V1)
cond<-dfvar[dfvar$VAR>3,]
View(cond)
View(df)
View(dfvar)
cond<-dfvar[dfvar$VAR>3 & !is.na(dfvar),]
View(cond)
cond<-dfvar[dfvar$VAR>3,]
View(cond)
cond<-dfvar[(dfvar$VAR>3),]
View(cond)
cond<-cond[!is.na(cond),]
View(cond)
cond<-cond[!is.na(cond$VAR),]
mediana<-median(df$MEAN)
mediana<-median(dfvar$MEAN)
mediana<-median(dfvar$MEANm , rm.na=T)
mediana<-median(dfvar$MEAN, rm.na=T)
mediana<-median(dfvar$MEAN, na.rm = T)
mediana<-mean(dfvar$MEAN, na.rm = T)
timelapse<-dfvar$DATE_RIFE[dfvar$VAR>3]
timelapse<-timelapse[!is.na(timelapse)]
length(timelapse)
split(dfvar, dfvar$VAR>3)
ciao<-split(dfvar, dfvar$VAR>3)
dfvar<-dfvar[2:(nrow(dfvar)-1,]
dfvar<-dfvar[2:(nrow(dfvar)-1),]
ciao<-split(dfvar, dfvar$VAR>3)
timelapse<-dfvar$DATE_RIFE[dfvar$VAR>3]
df<-pres[pres$WORK_ID==id,]
dfvar <- df %>%
arrange(DATE_RIFE) %>%
mutate(VAR = round(rollapply(V1, width = 3, FUN = sd, align = "center", fill = NA, by = 1),5),
MEAN = round(rollapply(V1, width = 3, FUN = mean, align = "center", fill = NA, by = 1),5)) %>%
ungroup()
dfvar<-dfvar[2:(nrow(dfvar)-1),]
media<-mean(dfvar$MEAN, na.rm = T)
timelapse<-dfvar$DATE_RIFE[dfvar$VAR>3]
for (i in 1:nrow(dfvar)) {
if (dfvar$DATE_RIFE[i] %in% timelapse){
if (dfvar$MEAN[i] > media) {
dfvar$Trigger<-1
} else {
dfvar$Trigger<-0
}
} else {
next()
}
}
View(dfvar)
media<-mean(dfvar$MEAN, na.rm = T)
media<-mean(dfvar$MEAN, na.rm = T)
timelapse<-dfvar$DATE_RIFE[dfvar$VAR>3]
for (i in 1:length(timelapse)) {
df_time<-dfvar[dfvar$DATE_RIFE>=timelapse[i] & dfvar$DATE_RIFE<=timelapse[i-1],]
if (mean(df_time$V1)>media){
df_time$trigger<-1
} else {
df_time$trigger<-0
}
}
i=1
df_time<-dfvar[dfvar$DATE_RIFE>=timelapse[i] & dfvar$DATE_RIFE<=timelapse[i-1],]
timelapse[i]
df_time<-dfvar[dfvar$DATE_RIFE>=timelapse[i] & dfvar$DATE_RIFE<=timelapse[i+1],]
View(df_time)
patternDist <- function(x,pattern){
apply(sign(x), 1,
function(row){
v <- row - pattern
v[is.na(pattern) & is.na(row)] <- 0
v[xor(is.na(pattern), is.na(row))] <- 1
sum( v != 0 )
})
}
x <- matrix(rnorm(40),nrow=10)
x[sample(40,4)] <- 0
x[sample(40,4)] <- NA
x <- as.data.frame(x)
View(x)
ciao<-patternDist(x)
View(df_time)
timelapse<-dfvar$DATE_RIFE[dfvar$VAR>3]
timelapse<-as.data.frame(dfvar$DATE_RIFE[dfvar$VAR>3])
View(timelapse)
install.packages(c('repr', 'IRdisplay', 'evaluate', 'crayon', 'pbdZMQ', 'devtools', 'uuid', 'digest'))
devtools::install_github('IRkernel/IRkernel')
IRkernel::installspec()
IRkernel::installspec()
tmp = installed.packages()
installedpackages = as.vector(tmp[is.na(tmp[,"Priority"]), 1])
save(installedpackages, file="C:\\Users/nbenvenuti/Desktop/installed_packages.rda")
load("C:\\Users/nbenvenuti/Desktop/installed_packages.rda")
install.packages(installedpackages)
library(ggplot2)
library(readr)
library(nortest)
plot(xs,wave.2,type="l",ylim=c(-1,1)); abline(h=0,lty=3)
xs <- seq(-2*pi,2*pi,pi/100)
wave.1 <- sin(3*xs)
wave.2 <- sin(10*xs)
par(mfrow = c(1, 2))
plot(xs,wave.1,type="l",ylim=c(-1,1)); abline(h=0,lty=3)
plot(xs,wave.2,type="l",ylim=c(-1,1)); abline(h=0,lty=3)
wave.3 <- 0.5 * wave.1 + 0.25 * wave.2
plot(xs,wave.3,type="l"); title("Eg complex wave"); abline(h=0,lty=3)
plot.fourier <- function(fourier.series, f.0, ts) {
w <- 2*pi*f.0
trajectory <- sapply(ts, function(t) fourier.series(t,w))
plot(ts, trajectory, type="l", xlab="time", ylab="f(t)"); abline(h=0,lty=3)
}
# An eg
plot.fourier(function(t,w) {sin(w*t)}, 1, ts=seq(0,1,1/100))
acq.freq <- 100                    # data acquisition frequency (Hz)
time     <- 6                      # measuring time interval (seconds)
ts       <- seq(0,time,1/acq.freq) # vector of sampling time-points (s)
f.0      <- 1/time                 # fundamental frequency (Hz)
dc.component       <- 0
component.freqs    <- c(3,10)      # frequency of signal components (Hz)
component.delay    <- c(0,0)       # delay of signal components (radians)
component.strength <- c(.5,.25)    # strength of signal components
f <- function(t,w) {
dc.component +
sum( component.strength * sin(component.freqs*w*t + component.delay))
}
plot.fourier(f,f.0,ts)
setwd("C:\\Users/nbenvenuti/Desktop/Tutorial PyR/Machine Learning/Part 3 - Classification/Section 17 - Kernel SVM/")
# Importing the dataset
dataset = read.csv('./Data/Social_Network_Ads.csv')
dataset = dataset[3:5]
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[1:2] = scale(training_set[1:2])
test_set[1:2] = scale(test_set[1:2])
# Fitting Logistic Regression to the Training set
library(e1071)
classifier = svm(formula = Purchased ~ .,
data = training_set,
type='C-classification',
kernel='radial')
y_pred = predict(classifier, newdata = test_set[1:2])
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
cm
library(ElemStatLearn)
set = training_set # Insert any dataset
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, 1:2],
main = 'Kernel SVM (Training set) Linear',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'dark blue', 'yellow'))
# Naive Bayes Classifier
setwd("C:\\Users/nbenvenuti/Desktop/Tutorial PyR/Machine Learning/Part 3 - Classification/Section 18 - Naive Bayes/")
# Importing the dataset
dataset = read.csv('./Data/Social_Network_Ads.csv')
dataset = dataset[3:5]
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[1:2] = scale(training_set[1:2])
test_set[1:2] = scale(test_set[1:2])
# Fitting Logistic Regression to the Training set
library(e1071)
classifier = naiveBayes(x = training_set[1:2],
y=training_set$Purchased)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[1:2])
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
dataset$Purchased = factor(dataset$Purchased,
levels = c(0,1))
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[1:2] = scale(training_set[1:2])
test_set[1:2] = scale(test_set[1:2])
# Fitting Logistic Regression to the Training set
library(e1071)
classifier = naiveBayes(x = training_set[1:2],
y=training_set$Purchased)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[1:2])
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
cm
library(ElemStatLearn)
set = training_set # Insert any dataset
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, 1:2],
main = 'Naive Bayes (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'dark blue', 'yellow'))
# Visualising the Test set results
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'Naive Bayes (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'dark blue', 'yellow'))
# Decision Tree Classification
setwd("C:\\Users/nbenvenuti/Desktop/Tutorial PyR/Machine Learning/Part 3 - Classification/Section 18 - Naive Bayes/")
# Importing the dataset
dataset = read.csv('./Data/Social_Network_Ads.csv')
dataset = dataset[3:5]
# Encoding The dependent Variable as factor
dataset$Purchased = factor(dataset$Purchased,
levels = c(0,1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[1:2] = scale(training_set[1:2])
test_set[1:2] = scale(test_set[1:2])
# Fitting Decision Tree Classification to the Training set
library(rpart)
classifier = rpart(formula = Purchased ~ .,
data=training_set)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[1:2])
y_pred = predict(classifier, newdata = test_set[1:2], type = 'class')
cm = table(test_set[, 3], y_pred)
cm
# Visualising the Training set results
library(ElemStatLearn)
set = training_set # Insert any dataset
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, 1:2],
main = 'Decision Tree Classification (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'dark blue', 'yellow'))
# Visualising the Test set results
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, -3],
main = 'Decision Tree Classification (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'dark blue', 'yellow'))
plot(classifier)
text(classifier)
summary(classifier)
printcp(classifier)
plotcp(classifier)
par(mfrow=c(1,2))
rsq.rpart(classifier)
tmp <- printcp(RegTree_train)
rsq.val <- 1-tmp[,c(3,4)]
tmp <- printcp(RegTree_train)
tmp <- printcp(classifier)
rpart.plot(x = classifier, yesno = 2, type = 0, extra = 0)
library(rpart)
rpart.plot(x = classifier, yesno = 2, type = 0, extra = 0)
library(rpart.plot)
rpart.plot(x = classifier, yesno = 2, type = 0, extra = 0)
par(mfrow=c(1,1))
library(rpart.plot)
rpart.plot(x = classifier, yesno = 2, type = 0, extra = 0)
tmp <- printcp(classifier)
rsq.val <- 1-tmp[,c(3,4)]
rsq.rpart(classifier)
tmp <- printcp(classifier)
setwd("C:\\Users/nbenvenuti/Desktop/Tutorial PyR/Machine Learning/Part 3 - Classification/Section 20 - Random Forest Classification/")
# Decision Tree Classification
setwd("C:\\Users/nbenvenuti/Desktop/Tutorial PyR/Machine Learning/Part 3 - Classification/Section 20 - Random Forest Classification/")
# Importing the dataset
dataset = read.csv('./Data/Social_Network_Ads.csv')
dataset = dataset[3:5]
# Encoding The dependent Variable as factor
dataset$Purchased = factor(dataset$Purchased,
levels = c(0,1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[1:2] = scale(training_set[1:2])
test_set[1:2] = scale(test_set[1:2])
library(randomForest)
library(randomForest)
classifier = randomForest(x= training_set[1:2],
y= training_set$Purchased,
ntree = 10)
y_pred = predict(classifier, newdata = test_set[1:2], type = 'class')
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
print(cm)
library(randomForest)
classifier = randomForest(x= training_set[1:2],
y= training_set$Purchased,
ntree = 100)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[1:2], type = 'class')
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
print(cm)
library(randomForest)
classifier = randomForest(x= training_set[1:2],
y= training_set$Purchased,
ntree = 10)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[1:2], type = 'class')
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
print(cm)
library(ElemStatLearn)
set = training_set # Insert any dataset
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, 1:2],
main = 'Random Forest Classification (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'dark blue', 'yellow'))
View(training_set)
library(randomForest)
classifier = randomForest(x= training_set[1:2],
y= training_set$Purchased,
ntree = 10)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[1:2], type = 'class')
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
print(cm)
# Visualising the Training set results
library(ElemStatLearn)
set = training_set # Insert any dataset
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, 1:2],
main = 'Random Forest Classification (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'dark blue', 'yellow'))
library(randomForest)
classifier = randomForest(x= training_set[1:2],
y= training_set$Purchased,
ntree = 500)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[1:2], type = 'class')
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
print(cm)
# Visualising the Training set results
library(ElemStatLearn)
set = training_set # Insert any dataset
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, 1:2],
main = 'Random Forest Classification (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'dark blue', 'yellow'))
plot(classifier)
text(classifier)
summary(classifier)
printcp(classifier)
plotcp(classifier)
rsq.rpart(classifier)
library(rpart.plot)
rsq.rpart(classifier)
library(randomForest)
classifier = randomForest(x= training_set[1:2],
y= training_set$Purchased,
ntree = 10)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[1:2], type = 'class')
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
print(cm)
# Plotting Decision Tree
plot(classifier)
# Decision Tree Classification
setwd("C:\\Users/nbenvenuti/Desktop/Tutorial PyR/Machine Learning/Part 4 - Clustering/Section 24 - K-Means Clustering/")
# Importing the dataset
dataset = read.csv('.\Data\Mall_Customers.csv')
# Decision Tree Classification
setwd("C:\\Users/nbenvenuti/Desktop/Tutorial PyR/Machine Learning/Part 4 - Clustering/Section 24 - K-Means Clustering/")
# Importing the dataset
dataset = read.csv('.\Data\Mall_Customers.csv')
dataset = read.csv('./Data/Mall_Customers.csv')
X <- dataset[4,5]
X <- dataset[4:5]
set.seed(6)
wcss <- vector()
for (i in (1:10)) wcss <- sum(kmeans(X,i)$withins)
plot(1:10, wcss, type='b', main="Cluset of Clients", xlab="Number of Cluster", ylab="WCSS")
set.seed(6)
wcss <- vector()
for (i in (1:10)) wcss[i] <- sum(kmeans(X,i)$withins)
plot(1:10, wcss, type='b', main="Cluset of Clients", xlab="Number of Cluster", ylab="WCSS")
set.seed(29)
kmeans <- kmeans(X, 5, iter.max = 300, nstart=10)
library(cluster)
library(cluster)
clusplot(X, kmeans$cluster,
lines = 0, shase=TRUE, color=TRUE, label=2, plotchar = FALSE,
main="Cluset of Clients", xlab="Number of Cluster", ylab="WCSS")
library(cluster)
clusplot(X, kmeans$cluster,
lines = 0, shade=TRUE, color=TRUE, label=2, plotchar = FALSE,
main="Cluset of Clients", xlab="Annual Income", ylab="Spending Score")
names<-c("ciao","ciao","ciao","ciao","ciao")
library(cluster)
clusplot(X, kmeans$cluster,
lines = 0, shade=TRUE, color=TRUE, label=2, plotchar = FALSE,
main="Cluset of Clients", xlab="Annual Income", ylab="Spending Score",
legend(names))
clusplot(X, kmeans$cluster,
lines = 0, shade=TRUE, color=TRUE, label=2, plotchar = FALSE,
main="Cluset of Clients", xlab="Annual Income", ylab="Spending Score",
text(names))
Dendrogram = hclust(dist(X, method = 'euclidean'), method='ward.D')
plot(Dendrogram,
main= 'Dendrogram',
xlab='Euclidean Distance',
ylab='Customers')
# Fitting Hierchical Clustering to the mall dataset
hc = hclust(dist(X, method = 'euclidean'), method='ward.D')
y_hc = cutree(hc, 5)
library(cluster)
clusplot(X, y_hc$cluster,
lines = 0, shade=TRUE, color=TRUE, label=2, plotchar = FALSE,
main="Cluset of Clients", xlab="Annual Income", ylab="Spending Score")
library(cluster)
clusplot(X, y_hc,
lines = 0, shade=TRUE, color=TRUE, label=2, plotchar = FALSE,
main="Cluset of Clients", xlab="Annual Income", ylab="Spending Score")
library(cluster)
clusplot(X, y_hc,
lines = 0, shade=TRUE, color=TRUE, label=2, plotchar = FALSE,
main="Cluster of Clients", xlab="Annual Income", ylab="Spending Score")
